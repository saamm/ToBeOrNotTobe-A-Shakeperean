{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import random\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"poem.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sonnet length: 83099.00 characters\n",
      "Minimum sonnet length: 6.00 characters\n",
      "Average sonnet length: 31191.00 characters\n"
     ]
    }
   ],
   "source": [
    "sonnets = raw_text.split('\\n\\n')\n",
    "sonnet_lens = [len(sonnet) for sonnet in sonnets]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar([i for i in range(1, len(sonnets)+1)], sonnet_lens)\n",
    "plt.title('Number of Characters per sonnet')\n",
    "plt.ylabel('Number of Characters')\n",
    "plt.xlabel('Sonnets')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Maximum sonnet length: %.2f characters' % np.max(sonnet_lens))\n",
    "print('Minimum sonnet length: %.2f characters' % np.min(sonnet_lens))\n",
    "print('Average sonnet length: %.2f characters' % np.mean(sonnet_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create mapping of unique characters to integers and integers to unique characters\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  93577\n",
      "Total Vocab:  36\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences: 31179\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen (40) characters\n",
    "\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(raw_text) - maxlen, step):\n",
    "    sentences.append(raw_text[i: i + maxlen])\n",
    "    next_chars.append(raw_text[i + maxlen])\n",
    "print('number of sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 36)                4644      \n",
      "=================================================================\n",
      "Total params: 89,124\n",
      "Trainable params: 89,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(Callback):\n",
    "\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "      # Function invoked at end of each epoch. Prints generated text.\n",
    "      print('')\n",
    "      base_dir = 'generated_text'\n",
    "      if not os.path.isdir(base_dir):\n",
    "          os.mkdir(base_dir)\n",
    "      \n",
    "      epoch_dir = os.path.join(base_dir, 'epoch_' + str(epoch))\n",
    "      if not os.path.isdir(epoch_dir):\n",
    "          os.mkdir(epoch_dir)\n",
    "\n",
    "      start_index = random.randint(0, len(raw_text) - maxlen - 1)\n",
    "      for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        \n",
    "          diversity_file = 'epoch' + str(epoch) + '_diversity=' + str(diversity) + '.txt'\n",
    "          file = open(os.path.join(epoch_dir, diversity_file), 'w')\n",
    "\n",
    "          generated = ''\n",
    "          sentence = raw_text[start_index: start_index + maxlen]\n",
    "          generated += sentence\n",
    "          \n",
    "          file.write(generated)\n",
    "\n",
    "          for i in range(600):\n",
    "              x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "              for t, char in enumerate(sentence):\n",
    "                  x_pred[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "              preds = model.predict(x_pred, verbose=0)[0]\n",
    "              next_index = sample(preds, diversity)\n",
    "              next_char = int_to_char[next_index]\n",
    "\n",
    "              sentence = sentence[1:] + next_char\n",
    "\n",
    "              file.write(next_char)\n",
    "        \n",
    "          print('diversity ' + str(diversity) + \" done.\")\n",
    "          file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumya.sinha\\model\n"
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "directory = 'model'\n",
    "if not os.path.isdir(directory):\n",
    "    os.mkdir(directory)\n",
    "%cd model\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss')\n",
    "callback = [checkpoint, myCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "244/244 [==============================] - 18s 69ms/step - loss: 2.4100\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 2/60\n",
      "244/244 [==============================] - 18s 74ms/step - loss: 1.9472\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 3/60\n",
      "244/244 [==============================] - 14s 57ms/step - loss: 1.7723\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 4/60\n",
      "244/244 [==============================] - 14s 57ms/step - loss: 1.6450\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 5/60\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 1.5477\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 6/60\n",
      "244/244 [==============================] - 13s 55ms/step - loss: 1.4549\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 7/60\n",
      "244/244 [==============================] - 13s 55ms/step - loss: 1.3710\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 8/60\n",
      "244/244 [==============================] - 16s 64ms/step - loss: 1.2982\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 9/60\n",
      "244/244 [==============================] - 13s 52ms/step - loss: 1.2345\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 10/60\n",
      "244/244 [==============================] - 13s 55ms/step - loss: 1.1790\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 11/60\n",
      "244/244 [==============================] - 13s 55ms/step - loss: 1.1374\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 12/60\n",
      "244/244 [==============================] - 14s 58ms/step - loss: 1.1030\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 13/60\n",
      "244/244 [==============================] - 13s 54ms/step - loss: 1.0731\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 14/60\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 1.0429\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 15/60\n",
      "244/244 [==============================] - 14s 56ms/step - loss: 1.0271\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 16/60\n",
      "244/244 [==============================] - 14s 55ms/step - loss: 1.0055\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 17/60\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.9876\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 18/60\n",
      "244/244 [==============================] - 13s 53ms/step - loss: 0.9693\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 19/60\n",
      "244/244 [==============================] - 14s 57ms/step - loss: 0.9539\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 20/60\n",
      "244/244 [==============================] - 13s 55ms/step - loss: 0.9446\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 21/60\n",
      "244/244 [==============================] - 14s 59ms/step - loss: 0.9263\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 22/60\n",
      "244/244 [==============================] - 14s 59ms/step - loss: 0.9144\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 23/60\n",
      "244/244 [==============================] - 15s 63ms/step - loss: 0.9038\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 24/60\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.8944\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 25/60\n",
      "244/244 [==============================] - 16s 67ms/step - loss: 0.8836\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 26/60\n",
      "244/244 [==============================] - 13s 55ms/step - loss: 0.8706\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 27/60\n",
      "244/244 [==============================] - 12s 51ms/step - loss: 0.8614\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 28/60\n",
      "244/244 [==============================] - 12s 51ms/step - loss: 0.8524\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 29/60\n",
      "244/244 [==============================] - 18s 73ms/step - loss: 0.8377\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 30/60\n",
      "244/244 [==============================] - 18s 75ms/step - loss: 0.8253\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 31/60\n",
      "244/244 [==============================] - 17s 71ms/step - loss: 0.8212\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 32/60\n",
      "244/244 [==============================] - 13s 54ms/step - loss: 0.8192\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 33/60\n",
      "244/244 [==============================] - 15s 63ms/step - loss: 0.7971\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 34/60\n",
      "244/244 [==============================] - 13s 54ms/step - loss: 0.7939\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 35/60\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.7847\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 36/60\n",
      "244/244 [==============================] - 17s 69ms/step - loss: 0.7738\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 37/60\n",
      "244/244 [==============================] - 14s 59ms/step - loss: 0.7691\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 38/60\n",
      "244/244 [==============================] - 14s 57ms/step - loss: 0.7596 0s \n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 39/60\n",
      "244/244 [==============================] - 14s 56ms/step - loss: 0.7507\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 40/60\n",
      "244/244 [==============================] - 12s 48ms/step - loss: 0.7407\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 41/60\n",
      "244/244 [==============================] - 12s 51ms/step - loss: 0.7370\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 42/60\n",
      "244/244 [==============================] - 12s 50ms/step - loss: 0.7295\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 43/60\n",
      "244/244 [==============================] - 12s 51ms/step - loss: 0.7252\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 44/60\n",
      "244/244 [==============================] - 13s 52ms/step - loss: 0.7137\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 45/60\n",
      "244/244 [==============================] - 12s 51ms/step - loss: 0.7047\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 46/60\n",
      "244/244 [==============================] - 18s 75ms/step - loss: 0.6963\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 47/60\n",
      "244/244 [==============================] - 13s 52ms/step - loss: 0.6910\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 48/60\n",
      "244/244 [==============================] - 13s 52ms/step - loss: 0.6818\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 49/60\n",
      "244/244 [==============================] - 13s 52ms/step - loss: 0.6803\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 50/60\n",
      "244/244 [==============================] - 13s 53ms/step - loss: 0.6751\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 51/60\n",
      "244/244 [==============================] - 12s 51ms/step - loss: 0.6749\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 52/60\n",
      "244/244 [==============================] - 12s 50ms/step - loss: 0.6688\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 53/60\n",
      "244/244 [==============================] - 11s 46ms/step - loss: 0.6619\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 54/60\n",
      "244/244 [==============================] - 13s 52ms/step - loss: 0.6489\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 55/60\n",
      "244/244 [==============================] - 12s 49ms/step - loss: 0.6415\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 56/60\n",
      "244/244 [==============================] - 12s 49ms/step - loss: 0.6435\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 57/60\n",
      "244/244 [==============================] - 12s 48ms/step - loss: 0.6308\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 58/60\n",
      "244/244 [==============================] - 14s 58ms/step - loss: 0.6284\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 59/60\n",
      "244/244 [==============================] - 14s 56ms/step - loss: 0.6267\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n",
      "Epoch 60/60\n",
      "244/244 [==============================] - 13s 52ms/step - loss: 0.6212\n",
      "\n",
      "diversity 0.2 done.\n",
      "diversity 0.5 done.\n",
      "diversity 1.0 done.\n",
      "diversity 1.2 done.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sonnet(temp):\n",
    "    ''' Given a temperature, generate a new sonnet '''\n",
    "    start_idx = np.random.randint(0, len(raw_text) - maxlen - 1)\n",
    "    new_sonnet = raw_text[start_idx:start_idx + maxlen]\n",
    "    print(new_sonnet)\n",
    "    for i in range(600):\n",
    "        # Vectorize generated text\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for j, char in enumerate(new_sonnet):\n",
    "            sampled[0, j, char_to_int[char]] = 1.\n",
    "\n",
    "        # Predict next character\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        pred_idx = sample(preds, temperature=temp)\n",
    "        next_char = chars[pred_idx]\n",
    "\n",
    "        # Append predicted character to seed text\n",
    "        new_sonnet += next_char\n",
    "        new_sonnet = new_sonnet[1:]\n",
    "\n",
    "        # Print to console\n",
    "        print(next_char, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writ,\n",
      "not making worse what nature made \n",
      "was pride.\n",
      "o, bears of their swory show whos the wronge?\n",
      "bow sweet best arturd to granes of such sees'\n",
      "cruel i take thoughts in the edsifter.\n",
      "that healthf of worthy shall i be which desert,\n",
      "for i in my love sweet berow whose done;\n",
      "but thou to the ehsseep, a stay, and i rest;\n",
      "but they love say thou sear of youthie pended\n",
      "thy forcor all i pride thee hat so will is due,\n",
      "but i so seet, stand os the winy beauty'st thee:\n",
      "when i dot false world deep in\n",
      "the world with thing; i not to thee that well such send,\n",
      "tires of their for what they rettant shall i despay\n",
      "and praises of their ride impence to thee"
     ]
    }
   ],
   "source": [
    "generate_sonnet(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ers to outlive long date.\n",
      "if my slight m\n",
      "y posir on the forsert of thy face:\n",
      "so to beaster's seem whose dare freion.\n",
      "love resare far for my despave make was summer's dief\n",
      "thy sake have proud cheek shall ruimies ante.\n",
      "thou seasing of their eart in her face.\n",
      "but no doing eyes are not thy disping sweet,\n",
      "which work the eash from how leppiss,\n",
      "with for thy dost to praise will king,\n",
      "or have's foo, and sho stall all weree?\n",
      "whook, whose fall's the world with thing.\n",
      "o, whose thoughts thee to see 'tilt to me.\n",
      "then the etress's dul purect of their ride\n",
      "which altern is 'finge to prove to eyes bessoold.\n",
      "but thy doth havion'd i farsed than the eyes"
     ]
    }
   ],
   "source": [
    "generate_sonnet(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thee, my loss is my love's gain,\n",
      "and los\n",
      "t all were where is the mother 'sade\n",
      "and take a 'tis and thou wearth they sand,\n",
      "or my groald'd, of hast thy love's ownwhness;\n",
      "and your your was i seem resir'd tring,\n",
      "and sink king anspiet death her worthy shame;\n",
      "both rost thou sear to great which chark\n",
      "caruth of griend a deadery, but not with new-c\n",
      "upliving,y and bose health, that is need mine,\n",
      "nor imblack bain lesse, surver muse unviled\n",
      "for him when i say, my love's sowler pain;\n",
      "how the world when thou rost have but your still.\n",
      "look's more from the rost far for my sake?\n",
      "yet all my lime hast thou dost subtr, your crunted, respoon scy live ?\n",
      "o,"
     ]
    }
   ],
   "source": [
    "generate_sonnet(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they see aright?\n",
      "if that be fair whereon\n",
      " you wilts of hist,\n",
      "and'd the dumbles makeed had gentle by mist:\n",
      "bain the filgs when i rea, sifpentlest wiy\n",
      "no ngriles so groat;\n",
      "for you wourd crunt otrught i make when show,\n",
      "the other of this fongsed, and go me, in more\n",
      "that etsift doth file his gaidsten thou ded,\n",
      "and who it thy phosouths best best sins,\n",
      "when i for my despive might if their restife.\n",
      "th.\n",
      "roses, in lost in love,\n",
      "and do not may out hold heart awree of loombrevall'\n",
      "love whithing of belf-al, decenvented,\n",
      "unthy lidies report, trom may chiod;\n",
      "but then i do baindning and ensure might imblyst\n",
      "b:\n",
      "what confort mine own well by ever ther"
     ]
    }
   ],
   "source": [
    "generate_sonnet(1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
